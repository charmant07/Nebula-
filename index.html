<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nebula Voice Interface</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- React & ReactDOM -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    
    <!-- Babel for JSX -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap');
        body { font-family: 'Inter', sans-serif; background-color: #050505; }
        
        /* Custom Scrollbar hide */
        .no-scrollbar::-webkit-scrollbar { display: none; }
        .no-scrollbar { -ms-overflow-style: none; scrollbar-width: none; }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        // (Icons and other UI code kept as before; truncated here with the same placeholders you used)
        const Icons = {
          X: () => <svg /* ... */></svg>,
          Mic: () => <svg /* ... */></svg>,
          MicOff: () => <svg /* ... */></svg>,
          Camera: () => <svg /* ... */></svg>,
          Sparkles: ({ className, size = 20 }) => <svg /* ... */></svg>,
          Power: () => <svg /* ... */></svg>
        };

        const App = () => {
          const [status, setStatus] = useState('idle');
          const [isOutputMuted, setIsOutputMuted] = useState(false);
          const [response, setResponse] = useState("");
          const [error, setError] = useState(null);
          const [conversationHistory, setConversationHistory] = useState(() => {
            try {
              const saved = localStorage.getItem('nebula-memory');
              return saved ? JSON.parse(saved) : [];
            } catch {
              return [];
            }
          });
          const [isWaitingForCheckIn, setIsWaitingForCheckIn] = useState(false);
          const [pendingAction, setPendingAction] = useState(null);
          const [visionMode, setVisionMode] = useState(false);

          // Keep apiKey empty in frontend — we use the Netlify function instead
          const apiKey = "";

          // refs
          const recognitionRef = useRef(null);
          const wakeWordRecognitionRef = useRef(null);
          const audioRef = useRef(null);
          const silenceTimerRef = useRef(null);
          const checkInTimerRef = useRef(null);
          const errorTimeoutRef = useRef(null);
          const isActiveRef = useRef(false);
          const isWakeListeningRef = useRef(false);
          const isMainRecognitionActive = useRef(false);
          const isWakeRecognitionActive = useRef(false);
          const videoRef = useRef(null);
          const canvasRef = useRef(null);

          // PCM -> WAV converter (unchanged)
          const pcmToWav = (base64Pcm, sampleRate = 24000) => {
            const binaryString = window.atob(base64Pcm);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) bytes[i] = binaryString.charCodeAt(i);
            const buffer = new ArrayBuffer(44 + bytes.length);
            const view = new DataView(buffer);
            view.setUint32(0, 0x52494646, false);
            view.setUint32(4, 36 + bytes.length, true);
            view.setUint32(8, 0x57415645, false);
            view.setUint32(12, 0x666d7420, false);
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            view.setUint32(36, 0x64617461, false);
            view.setUint32(40, bytes.length, true);
            for (let i = 0; i < bytes.length; i++) view.setUint8(44 + i, bytes[i]);
            return new Blob([buffer], { type: 'audio/wav' });
          };

          const clearAllTimers = () => {
            [silenceTimerRef, checkInTimerRef, errorTimeoutRef].forEach(ref => {
              if (ref.current) {
                clearTimeout(ref.current);
                ref.current = null;
              }
            });
          };

          const setErrorWithTimeout = (msg) => {
            setError(msg);
            if (errorTimeoutRef.current) clearTimeout(errorTimeoutRef.current);
            errorTimeoutRef.current = setTimeout(() => setError(null), 5000);
          };

          // Unlock audio on first user gesture to reduce autoplay blocking
          useEffect(() => {
            const unlockAudioOnGesture = async () => {
              try {
                if (window.AudioContext || window.webkitAudioContext) {
                  const AudioCtx = window.AudioContext || window.webkitAudioContext;
                  const audioCtx = new AudioCtx();
                  if (audioCtx.state === 'suspended') {
                    await audioCtx.resume();
                  }
                }
                const a = document.createElement('audio');
                a.src = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAIA+AAACABAAZGF0YQAAAAA=';
                try { await a.play(); } catch(e) { /* ignore */ }
              } catch (e) {
                console.warn('unlockAudioOnGesture failed', e);
              }
            };
            const onFirstClick = () => unlockAudioOnGesture();
            document.addEventListener('click', onFirstClick, { once: true, passive: true });
            return () => document.removeEventListener('click', onFirstClick);
          }, []);

          // (detectCommand, executeAction, handleCommand, timers — unchanged; keep your existing logic)

          const detectCommand = (text) => {
            // ... same as your existing detectCommand implementation ...
            const lower = text.toLowerCase();
            if (lower.includes('set alarm') || lower.includes('wake me')) {
              const timeMatch = lower.match(/(\d{1,2})\s*(am|pm|o'clock)|(\d{1,2}):(\d{2})\s*(am|pm)?/i);
              return { type: 'alarm', raw: text, intent: 'set_alarm', params: { time: timeMatch ? timeMatch[0] : null } };
            }
            // other checks ...
            if (lower.includes('yes') || lower.includes('yeah') || lower.includes('confirm') || lower.includes('correct')) return { type: 'confirm', raw: text, intent: 'confirm_action' };
            if (lower.includes('no') || lower.includes('cancel') || lower.includes('never mind')) return { type: 'cancel', raw: text, intent: 'cancel_action' };
            return null;
          };

          const executeAction = async (command) => {
            // same as your existing executeAction
            try {
              let result;
              switch (command.type) {
                case 'alarm': result = await { success: true, message: `Alarm set for ${command.params.time || '7 AM'}` }; break;
                case 'timer': result = await { success: true, message: `Timer set for ${command.params.duration || '5 minutes'}` }; break;
                case 'reminder': result = await { success: true, message: `Reminder: ${command.params.what || 'task'}` }; break;
                case 'search': result = await { success: true, message: `Searching for "${command.params.query}"` }; break;
                case 'weather': result = await { success: true, message: "It's 72°F and sunny" }; break;
                default: result = { success: false, message: "Unknown command" };
              }
              return result;
            } catch (err) {
              return { success: false, message: "Action failed" };
            }
          };

          // speech recognition setup unchanged (keeps the same logic you already have)
          useEffect(() => {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
              recognitionRef.current = new SpeechRecognition();
              recognitionRef.current.continuous = false;
              recognitionRef.current.interimResults = false;
              recognitionRef.current.lang = 'en-US';

              recognitionRef.current.onstart = () => { isMainRecognitionActive.current = true; setStatus('listening'); };
              recognitionRef.current.onend = () => { 
                isMainRecognitionActive.current = false; 
                if (isActiveRef.current && (status === 'listening' || status === 'active')) {
                  setTimeout(() => startListening(), 300);
                }
              };

              recognitionRef.current.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                clearAllTimers();
                if (isWaitingForCheckIn) setIsWaitingForCheckIn(false);
                const command = detectCommand(transcript);
                if (command) handleCommand(command);
                else getGeminiResponse(transcript);
              };

              recognitionRef.current.onerror = (event) => {
                if (event.error === 'not-allowed') {
                  setErrorWithTimeout("Mic access needed");
                  handleEndConversation();
                }
              };

              wakeWordRecognitionRef.current = new SpeechRecognition();
              wakeWordRecognitionRef.current.continuous = true;
              wakeWordRecognitionRef.current.interimResults = false;
              wakeWordRecognitionRef.current.lang = 'en-US';

              wakeWordRecognitionRef.current.onstart = () => { isWakeRecognitionActive.current = true; setStatus('wakeListen'); };
              wakeWordRecognitionRef.current.onend = () => { 
                isWakeRecognitionActive.current = false; 
                if (isWakeListeningRef.current) setTimeout(() => startWakeWordListening(), 300);
              };

              wakeWordRecognitionRef.current.onresult = (event) => {
                const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
                if (transcript.includes('hey nebula') || transcript.includes('nebula')) {
                  isWakeListeningRef.current = false;
                  wakeWordRecognitionRef.current?.stop();
                  handleStartConversation();
                }
              };
            }
            return () => clearAllTimers();
          }, [status]);

          // Start/stop helpers unchanged
          const startSilenceTimer = () => {
            clearAllTimers();
            silenceTimerRef.current = setTimeout(() => {
              if (isActiveRef.current && !isWaitingForCheckIn) handleSilenceTimeout();
            }, 60000);
          };

          const handleSilenceTimeout = () => {
            if (!isActiveRef.current) return;
            setIsWaitingForCheckIn(true);
            setStatus('checking');
            const msg = "Hey, you still there?";
            setResponse(msg);
            speakResponse(msg, true);
            checkInTimerRef.current = setTimeout(() => handleEndConversation(), 30000);
          };

          const startListening = () => {
            if (!isActiveRef.current || isMainRecognitionActive.current) return;
            try { recognitionRef.current && recognitionRef.current.start(); } catch (err) { console.warn("Main recognition start conflict", err); }
          };

          const startWakeWordListening = () => {
            if (!isWakeListeningRef.current || isWakeRecognitionActive.current) return;
            try { wakeWordRecognitionRef.current && wakeWordRecognitionRef.current.start(); } catch (err) { console.warn("Wake word recognition start conflict", err); }
          };

          // speakResponse now uses the Netlify proxy for TTS and handles audio.play() promise
          const speakResponse = async (text, isCheckIn = false) => {
            if (isOutputMuted) {
              if (!isCheckIn) startSilenceTimer();
              if (isActiveRef.current) startListening();
              return;
            }

            try {
              console.debug('Requesting TTS for text:', text);
              const ttsUrl = '/.netlify/functions/gemini-proxy?model=gemini-2.5-flash-preview-tts';
              const res = await fetch(ttsUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  contents: [{ parts: [{ text: `Say naturally: ${text}` }] }],
                  generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Aoede" } } }
                  }
                })
              });

              if (!res.ok) {
                const txt = await res.text();
                console.error('TTS proxy returned non-ok', res.status, txt);
                setErrorWithTimeout('TTS request failed');
                if (isActiveRef.current) startListening();
                return;
              }

              const result = await res.json();
              const pcmData = result.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
              if (pcmData) {
                setStatus('speaking');
                const wavBlob = pcmToWav(pcmData, 24000);
                const audioUrl = URL.createObjectURL(wavBlob);
                if (audioRef.current) {
                  audioRef.current.src = audioUrl;
                  try {
                    await audioRef.current.play();
                  } catch (playErr) {
                    console.warn('audio.play() failed:', playErr);
                    setErrorWithTimeout('Audio playback blocked — tap the page to allow audio.');
                    // If playback blocked, resume listening to keep UX responsive
                    if (isActiveRef.current) startListening();
                    URL.revokeObjectURL(audioUrl);
                    return;
                  }
                  audioRef.current.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    if (isActiveRef.current) {
                      if (!isCheckIn) startSilenceTimer();
                      startListening();
                    } else setStatus('idle');
                  };
                }
              } else {
                console.warn('TTS: no pcmData returned; falling back to text-only behaviour');
                if (isActiveRef.current) startListening();
              }
            } catch (err) {
              console.error('speakResponse caught error:', err);
              setErrorWithTimeout('TTS connection error');
              if (isActiveRef.current) startListening();
            }
          };

          // getGeminiResponse uses the Netlify proxy for generative text
          const getGeminiResponse = async (userInput) => {
            setStatus('thinking');
            setResponse("");

            let imageBase64 = null;
            if (visionMode && canvasRef.current && videoRef.current) {
              const context = canvasRef.current.getContext('2d');
              canvasRef.current.width = videoRef.current.videoWidth;
              canvasRef.current.height = videoRef.current.videoHeight;
              context.drawImage(videoRef.current, 0, 0);
              imageBase64 = canvasRef.current.toDataURL('image/png').split(',')[1];
            }

            const currentTurn = {
              role: "user",
              parts: [
                { text: userInput },
                ...(imageBase64 ? [{ inlineData: { mimeType: "image/png", data: imageBase64 } }] : [])
              ]
            };

            const newHistory = [...conversationHistory, currentTurn];

            try {
              console.debug('Requesting generation for:', userInput);
              const genUrl = '/.netlify/functions/gemini-proxy?model=gemini-2.5-flash-preview-09-2025';
              const res = await fetch(genUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  contents: newHistory,
                  systemInstruction: { parts: [{ text: "You are Nebula, Patrick's AI assistant. Conversational and witty. Keep it 2-4 sentences. No markdown." }] },
                  tools: [{ googleSearch: {} }]
                })
              });

              if (!res.ok) {
                const txt = await res.text();
                console.error('Generation proxy returned non-ok', res.status, txt);
                setErrorWithTimeout('Generation failed');
                if (isActiveRef.current) startListening();
                return;
              }

              const data = await res.json();
              const text = data.candidates?.[0]?.content?.parts?.[0]?.text || "I couldn't process that.";
              const updatedHistory = [...newHistory, { role: "model", parts: [{ text: text }] }];
              setConversationHistory(updatedHistory);
              localStorage.setItem('nebula-memory', JSON.stringify(updatedHistory));
              setResponse(text);
              speakResponse(text);
            } catch (err) {
              console.error('getGeminiResponse caught error:', err);
              setErrorWithTimeout("Connection issue");
              if (isActiveRef.current) startListening();
            }
          };

          const handleStartConversation = () => {
            isWakeListeningRef.current = false;
            wakeWordRecognitionRef.current?.stop();
            isActiveRef.current = true;
            startListening();
            startSilenceTimer();
          };

          const handleEndConversation = () => {
            isActiveRef.current = false;
            isWakeListeningRef.current = false;
            clearAllTimers();
            try { recognitionRef.current?.stop(); } catch(e){}
            try { wakeWordRecognitionRef.current?.stop(); } catch(e){}
            if (audioRef.current) { audioRef.current.pause(); audioRef.current.currentTime = 0; }
            setStatus('idle');
            setResponse("");
          };

          const handleWakeWordMode = () => {
            isWakeListeningRef.current = true;
            startWakeWordListening();
          };

          return (
            <div className="flex h-screen w-full bg-[#050505] text-white font-sans overflow-hidden items-center justify-center">
              <audio ref={audioRef} className="hidden" />
              <canvas ref={canvasRef} className="hidden" />
              
              <video 
                ref={videoRef} 
                autoPlay 
                muted 
                playsInline
                className={`absolute inset-0 object-cover w-full h-full transition-opacity duration-1000 ${visionMode ? 'opacity-20 grayscale' : 'opacity-0'}`} 
              />
              
              <div className={`absolute top-[12%] flex flex-col items-center transition-all ${status === 'idle' ? 'opacity-20' : 'opacity-100'}`}>
                <p className="text-xs font-light tracking-[0.4em] uppercase text-white/60">
                  {error || (status === 'idle' ? 'Dormant' : status === 'wakeListen' ? 'Say "Hey Nebula"' : status)}
                </p>
                {response && (status === 'speaking' || status === 'checking') && (
                  <p className="mt-3 text-center max-w-sm text-white/40 italic text-sm leading-relaxed px-6">"{response}"</p>
                )}
              </div>

              <div 
                className="relative w-64 h-64 md:w-80 md:h-80 cursor-pointer" 
                onClick={() => { if (status === 'idle') handleWakeWordMode(); else if (status === 'wakeListen') handleStartConversation(); }}
              >
                {/* Visual UI kept unchanged */}
                <div className={`absolute inset-0 rounded-full blur-[120px] transition-all duration-1000 scale-150 ${
                  status === 'speaking' ? 'bg-blue-500/30' : 
                  status === 'listening' ? 'bg-indigo-500/30' : 
                  status === 'thinking' ? 'bg-purple-500/20' : 
                  status === 'wakeListen' ? 'bg-green-500/20' : 'bg-transparent'
                }`} />
                <svg className="w-full h-full" viewBox="0 0 200 200">
                  <defs>
                    <filter id="goo"><feGaussianBlur in="SourceGraphic" stdDeviation="8" result="blur" /><feColorMatrix in="blur" mode="matrix" values="1 0 0 0 0  0 1 0 0 0  0 0 1 0 0  0 0 0 19 -9" /></filter>
                  </defs>
                  <g filter="url(#goo)">
                    <circle cx="100" cy="100" r="40" className={`transition-all duration-1000 ${status === 'idle' ? 'fill-white opacity-5 scale-75' : 'fill-white opacity-100'}`} />
                    {status !== 'idle' && (
                      <>
                        <circle cx="100" cy="100" r="35" className="fill-blue-400/90 animate-blob-slow" />
                        <circle cx="100" cy="100" r="30" className="fill-indigo-400/80 animate-blob-fast" />
                      </>
                    )}
                  </g>
                </svg>
              </div>

              {(status !== 'idle' && status !== 'wakeListen') && (
                <button onClick={handleEndConversation} className="absolute top-8 right-8 p-4 rounded-full bg-red-500/5 border border-red-500/10 text-red-400/50 hover:text-red-400 transition-all"><Icons.X /></button>
              )}

              <div className="absolute bottom-12 flex flex-col items-center gap-6">
                <div className="flex items-center gap-8 opacity-50 hover:opacity-100 transition-all">
                  <button onClick={() => setIsOutputMuted(!isOutputMuted)} className={`p-4 rounded-full border border-white/5 bg-white/5 backdrop-blur-xl ${isOutputMuted ? 'text-red-500' : 'text-white/40'}`}><Icons.MicOff /></button>
                  <button onClick={() => setVisionMode(!visionMode)} className={`p-4 rounded-full border border-white/5 bg-white/5 backdrop-blur-xl ${visionMode ? 'text-indigo-400' : 'text-white/40'}`}><Icons.Camera /></button>
                  <button onClick={() => { handleEndConversation(); setConversationHistory([]); localStorage.removeItem('nebula-memory'); }} className="p-4 rounded-full border border-white/5 bg-white/5 backdrop-blur-xl"><Icons.Power /></button>
                </div>
                <div className="flex items-center gap-2 px-4 py-2 rounded-full bg-white/5 border border-white/10 opacity-30"><Icons.Sparkles size={12} className="text-cyan-400" /><span className="text-sm">Nebula</span></div>
              </div>

              <style dangerouslySetInnerHTML={{ __html: `
                @keyframes blob-slow { 0%, 100% { transform: translate(0, 0) scale(1); } 33% { transform: translate(20px, -15px) scale(1.1); } 66% { transform: translate(-15px, 20px) scale(0.9); } }
                @keyframes blob-fast { 0%, 100% { transform: translate(0, 0) scale(1.1); } 50% { transform: translate(15px, 15px) scale(0.9); } }
                .animate-blob-slow { animation: blob-slow 10s ease-in-out infinite; transform-origin: center; }
                .animate-blob-fast { animation: blob-fast 6s ease-in-out infinite; transform-origin: center; }
              `}} />
            </div>
          );
        };

        const root = ReactDOM.createRoot(document.getElementById('root'));
        root.render(<App />);
    </script>
</body>
</html>
